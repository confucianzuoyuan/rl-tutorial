# 第一章 强化学习中的关键概念

> [!NOTE]
> 本章内容：
> - 强化学习使用的数学语言和数学符号
> - 对强化学习算法的作用进行高层次的解释（先不做代码实现，目标是有一个大概的了解）
> - 算法背后的一些核心数学知识

简而言之，强化学习（RL）是研究代理及其如何通过反复试验进行学习的学科。它形式化地阐述了这样一种观点：对代理的行为进行奖励或惩罚，会增加其在未来重复或放弃该行为的可能性。

**AlphaGo**

下棋的程序是 **代理** 。在训练下棋的程序时，对下的一步好棋进行奖励，对下的一步臭棋进行惩罚，最终训练出一个超越人类的棋手。

**基于人类反馈的强化学习（RLHF）**

大语言模型作为 **代理** 。对大语言模型输出的好的回答进行奖励，对大语言模型输出的不好的回答进行惩罚。进而改变大语言模型的参数（微调），来让大语言模型的输出能够对齐到人类的偏好。

## 关键概念和术语

![](./images/rl_diagram_transparent_bg.png)

![](./images/1.png)

强化学习的主要角色是 **代理** 和 **环境** 。环境是代理生存并与之交互的世界。在交互的每一步，代理都会观察（可能是部分）世界的状态，然后决定采取的行动。环境会随着代理的行动而变化，但也可能自行变化。

代理还会感知来自环境的 **奖励** 信号，这是一个数值，用来告诉代理当前世界状态的好坏。代理的目标是最大化其累积奖励，即 **回报** 。强化学习方法是代理学习行为以实现其目标的方法。

为了更具体地讨论强化学习的作用，我们需要引入一些额外的术语。我们需要讨论

- 状态（State）和观察（Observation）
- 动作空间（Action Space）
- 策略（Policy）
- 轨迹（Trajectory）
- 不同的回报（Return）公式
- 强化学习优化问题
- 价值函数（Value Function）

### 状态和观察

状态 $s$ 是对世界状态的完整描述。状态中不存在任何隐藏于世界之外的信息。 观察 $o$ 是对状态的部分描述，可能会遗漏一些信息。

TODO：添加图片

- 对于棋类，我们可以获取世界状态的完整描述，因为棋盘没有任何隐藏信息。
- 对于超级玛丽游戏，我们只能看到玩家所处的画面，所以只能叫做“观察”。

在深度强化学习中，我们几乎总是用向量、矩阵或张量来表示状态和观察值。例如，视觉观察可以用其像素值的 RGB 矩阵表示；机器人的状态可以用其关节角度和速度表示。

当代理能够观察到环境的完整状态时，我们称该环境是 **完全可观察的** 。当代理只能看到部分观察结果时，我们称该环境是 **部分可观察的** 。

> [!NOTE]
> **必须掌握的概念**
> 强化学习符号有时会将表示状态的符号 $s$ 放在技术上更适合表示观察的符号 $o$ 的位置。具体来说，这种情况发生在讨论代理如何决定某个动作时：我们经常在符号中表示该动作取决于状态，但实际上，由于代理无法访问状态，因此该动作取决于观察。
>
> 具体根据上下文自行判断。

### 动作空间

不同的环境允许不同类型的动作。给定环境中所有有效动作的集合通常称为动作空间 。某些环境，例如围棋，具有 **离散的动作空间** ，其中代理只能进行有限数量的移动。其他环境，例如代理在物理世界中控制机器人的环境，具有 **连续的动作空间** 。在连续空间中，动作是向量、矩阵或者张量。

- 离散动作空间：超级玛丽中玩家只有“起跳”，“蹲下”等有限的几个动作。
- 连续动作空间：自动驾驶，方向盘旋转1度，1.1度，1.2度，......，有无限多种动作。

这种区别对深度强化学习的方法有着相当深远的影响。一些算法只能在一种情况下直接应用，而对于另一种情况则需要进行大量的重新设计。

### 策略

**策略** 是代理用来决定采取哪些行动的规则。它可以是确定性的，在这种情况下通常用 $\mu$ 表示：

$$
a_t=\mu(s_t)
$$

或者策略也可能是随机的，在这种情况下它通常用 $\pi$ 表示：

$$
a_t \sim \pi(\cdot | s_t)
$$

因为策略本质上是代理的大脑，所以用“策略”一词代替“代理”并不罕见，例如说“策略试图最大化奖励”。

在深度强化学习中，我们处理参数化策略：这些策略的输出是可计算函数，取决于一组参数（例如神经网络的权重和偏置），我们可以通过某种优化算法来调整这些参数以改变行为。

我们常常用 $\theta$ 或 $\phi$ 来表示这种策略的参数，然后将其写为策略符号的下标，以突出这种联系：

$$
\begin{split}
a_t &= \mu_{\theta}(s_t) \\
a_t &\sim \pi_{\theta}(\cdot | s_t).
\end{split}
$$

### 确定性策略

```py
pi_net = nn.Sequential(
    nn.Linear(obs_dim, 64),
    nn.Tanh(),
    nn.Linear(64, 64),
    nn.Tanh(),
    nn.Linear(64, act_dim)
)
```

这将构建一个多层感知机 (MLP) 网络，该网络具有两个大小为 64 的隐藏层和 $\tanh$ 激活函数。如果 `obs` 是一个包含一批观测值的 `Numpy` 数组，则可以使用 `pi_net` 获取一批操作，如下所示：

```py
obs_tensor = torch.as_tensor(obs, dtype=torch.float32)
actions = pi_net(obs_tensor)
```

也就是当 `obs_tensor` 张量确定的情况下，MLP 的输出一定是确定的。

### 随机策略

深度强化学习中最常见的两种随机策略是 **分类策略** 和 **对角高斯策略** 。

对于使用和训练随机策略来说，两个关键计算至关重要：

- 从策略中抽样动作，
- 并计算特定动作的对数似然， $\log \pi_{\theta}(a|s)$ 。

接下来，我们将描述如何针对分类策略和对角高斯策略执行这些操作。

> [!NOTE]
> 分类策略
> 分类策略就像一个针对离散动作的分类器。构建分类策略的神经网络的方式与构建分类器的方式相同：输入是观察值，然后是若干层（可能是卷积层或全连接层，具体取决于输入的类型），最后是最后一个线性层，为每个动作提供 `logits` ，最后使用 `softmax` 将 `logits` 转换为概率。
>
> 采样。给定每个动作的概率，PyTorch 等框架内置了采样工具。
>
> 对数似然。将最后一层概率表示为 $P_{\theta}(s)$ 。它是一个向量，其元素数量与动作数量相同，因此我们可以将动作视为该向量的索引。然后，可以通过对向量进行索引来获得动作 $a$ 的对数似然：
>
> $$
> \log \pi_{\theta}(a|s) = \log \left[P_{\theta}(s)\right]_a
> $$
>
> 对角高斯策略
> 多元高斯分布（也可以称之为多元正态分布）由均值向量 $\mu$ 和协方差矩阵 $\Sigma$ 描述。对角高斯分布是一种特殊情况，其协方差矩阵仅在对角线上有元素。因此，我们可以用向量来表示它。
>
> 对角高斯策略始终具有一个从观测值映射到平均动作 $\mu_{\theta}(s)$ 的神经网络。协方差矩阵通常有两种不同的表示方式。
>
> - 第一种方式：只有一个对数标准差向量 $\log \sigma$ ，它不是状态函数： $\log \sigma$ 是独立参数。（PPO 算法的实现就是这样的。）
> - 第二种方式：有一个神经网络，可以将状态映射到对数标准差 $\log \sigma_{\theta}(s)$ 。它可以选择与均值网络共享一些层。
> 请注意，在这两种情况下，我们输出的是对数标准差，而不是直接输出标准差。这是因为对数标准差可以取 $(-\infty, \infty)$ 中的任意值，而标准差必须为非负值。如果不必强制执行这些约束，训练参数会更容易。标准差可以通过对数标准差取幂直接得出，因此用这种方式表示它们不会有任何损失。
> 采样。给定平均动作 $\mu_{\theta}(s)$ 和标准差 $\sigma_{\theta}(s)$ ，以及球面高斯 $( z \sim \mathcal{N}(0, I) )$ 的噪声向量 $z$ ，可以使用以下公式计算动作样本
> $$
> a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z
> $$
> 其中 $\odot$ 表示两个向量的元素乘积。标准框架内置了生成噪声向量的方法，例如 `torch.normal` 。或者，可以构建分布对象，例如通过 `torch.distributions.Normal` ，并使用它们生成样本。（后一种方法的优势在于，这些对象还可以计算对数似然函数。）
> 对数似然。对于均值为 $\mu = \mu_{\theta}(s)$ 、标准差为 $\sigma = \sigma_{\theta}(s)$ 的对角高斯分布， $k$ 维动作 $a$ 的对数似然由下式给出：
> $$
> \log \pi_{\theta}(a|s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right)
> $$

### 轨迹

轨迹 $\tau$ 是世界上一系列的状态和动作，

$$
\tau = (s_0, a_0, s_1, a_1, ...)
$$

世界的第一个状态 $s_0$ 是从起始状态分布中随机抽取的，有时表示为 $\rho_0$ ：

$$
s_0 \sim \rho_0(\cdot)
$$

状态转换（即在时间 $t$ , $s_t$ 处的状态与时间 $t+1$ , $s_{t+1}$ 处的状态之间世界所发生的变化）受环境的自然规律支配，并且仅取决于最近的动作 $a_t$ 。它们可以是确定性的，

$$
s_{t+1} = f(s_t, a_t)
$$

或随机的，

$$
s_{t+1} \sim P(\cdot|s_t, a_t)
$$

代理会根据其策略采取相应行动。

### 奖励和回报（Reward and Return）

奖励函数 $R$ 在强化学习中至关重要。它取决于世界的当前状态、刚刚采取的行动以及世界的下一个状态：

$$
r_t = R(s_t, a_t, s_{t+1})
$$

尽管这通常被简化为仅依赖于当前状态 $r_t = R(s_t)$ 或状态-动作对 $r_t = R(s_t,a_t)$ 。

代理的目标是最大化轨迹上的累积奖励，但这实际上可能意味着几件事。我们将用 $R(\tau)$ 来表示所有这些情况，这样一来，上下文就能清楚地表明我们指的是哪种情况，或者这无关紧要（因为相同的方程式适用于所有情况）。

一种回报是 **有限期限的且没有折扣的回报** ，它只是在固定步骤窗口内获得的奖励的总和：

$$
R(\tau) = \sum_{t=0}^T r_t
$$

另一种回报是 **无限期的且有折扣的回报** ，它是代理曾经获得的所有奖励的总和，但会根据未来获得奖励的时间对奖励打折扣。此奖励公式包含一个折扣因子 $\gamma \in (0,1)$ ：

$$
R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t
$$

但我们为什么要用折扣因子呢？难道我们不是只想获得所有奖励吗？我们当然想，但折扣因子既直观又数学上方便。直观上来说：现在的现金比以后的现金更好。数学上来说：无限期的奖励总和可能不会收敛到一个有限值，而且很难用方程式来处理。但是，有了折扣因子，并且在合理的条件下，无限期的奖励总和就会收敛。

现在就能获得 1 万元，和一百年以后能获得 1 亿元，我们会选择哪一个选项呢？我们会选现在获得 1 万元，因为折扣因子的存在，一百年以后的奖励对现在而言衰减到了几乎为 0 。

虽然在 RL 的数学公式上，这两种回报公式之间的界限非常明显，但深度 RL 实践往往会模糊这条界限——例如​​，我们经常设置算法来优化没有折扣的回报，但在估计价值函数时使用折扣因子。

### 强化学习问题

无论选择何种回报衡量标准（无论是无限期折扣还是有限期不折扣），也无论选择何种策略，RL 中的目标都是选择一种策略，当代理按照该策略行事时，该策略可以最大化 **预期回报** 。

要谈论预期回报，我们首先必须谈论轨迹的概率分布。

假设环境转换和策略都是随机的。在这种情况下， $T$ 步轨迹的概率为：

$$
P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t)
$$

预期回报（无论采用哪种衡量标准）用 $J(\pi)$ 表示，即：

$$
J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau) = \underE{\tau\sim \pi}{R(\tau)}
$$

强化学习中的核心优化问题可以表示为

$$
\pi^* = \arg \max_{\pi} J(\pi)
$$

其中 $\pi^*$ 为最优策略 。

### 价值函数

了解状态或状态-动作对的 **价值** 通常很有用。我们所说的价值是指从该状态或状态-动作对开始，然后一直按照特定策略行动的预期回报。几乎每种强化学习算法都会以某种方式使用 **价值函数** 。

这里有四个主要值得注意的函数。

1. On-Policy价值函数 $V^{\pi}(s)$ ，如果从状态 $s$ 开始并始终按照策略 $\pi$ 行事，它将给出预期回报：

$$
V^{\pi}(s) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}
$$

2. On-Policy动作-价值函数 $Q^{\pi}(s,a)$ ，如果从状态 $s$ 开始，采取任意动作 $a$ （可能不是来自策略），然后永远按照策略 $\pi$ 采取行动，它将给出预期的回报：

$$
Q^{\pi}(s,a) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}
$$

3. 最优价值函数 $V^*(s)$ ，如果从状态 $s$ 开始并始终按照环境中的最优策略行事，它将给出预期回报：

$$
V^*(s) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}
$$

4. 最优动作价值函数 $Q^*(s,a)$ ，如果从状态 $s$ 开始，采取任意动作 $a$ ，然后永远按照环境中的最优策略采取行动，它将给出预期回报：

$$
Q^*(s,a) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}
$$

当我们讨论价值函数时，如果不考虑时间依赖性，我们仅仅指无限期预期有折扣的收益 。有限期没有折扣的收益的价值函数需要接受时间作为参数。你能想想为什么吗？提示：时间到了会发生什么？

价值函数和动作-价值函数之间有两个经常出现的关键联系：

$$
V^{\pi}(s) = \underE{a\sim \pi}{Q^{\pi}(s,a)}
$$

和

$$
V^*(s) = \max_a Q^* (s,a)
$$

这些关系直接遵循刚刚给出的定义：你能证明它们吗？

### 最优 Q 函数和最优动作

最优动作-价值函数 $Q^*(s,a)$ 与最优策略选择的动作之间存在重要联系。根据定义， $Q^*(s,a)$ 给出了从状态 $s$ 开始，采取（任意）动作 $a$ ，然后始终按照最优策略行动的预期回报。

$s$ 中的最优策略将选择能够最大化从 $s$ 开始的预期回报的行动。因此，如果我们有 $Q^*$ ，我们就可以通过以下方式直接获得最优行动 $a^*(s)$

$$
a^*(s) = \arg \max_a Q^* (s,a)
$$

注意：可能有多个动作可以最大化 $Q^*(s,a)$ ，在这种情况下，所有动作都是最优的，而最优策略可能会随机选择其中任何一个动作。但总有一个最优策略可以确定性地选择一个动作。

### 贝尔曼方程

这四个价值函数都遵循特殊的自洽方程，称为 **贝尔曼方程** 。贝尔曼方程背后的基本思想是：

> 起点的价值是您期望从那里获得的回报，加上下一步的价值。

$$
\begin{align*}
V^{\pi}(s) &= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\
Q^{\pi}(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}},
\end{align*}
$$

其中 $s' \sim P$ 是 $s' \sim P(\cdot |s,a)$ 的简写，表示下一个状态 $s'$ 是从环境的转换规则中采样而来的； $a \sim \pi$ 是 $a \sim \pi(\cdot|s)$ 的简写； $a' \sim \pi$ 是 $a' \sim \pi(\cdot|s')$ 的简写。

最优值函数的贝尔曼方程为

$$
\begin{align*}
V^*(s) &= \max_a \underE{s'\sim P}{r(s,a) + \gamma V^*(s')}, \\
Q^*(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \max_{a'} Q^*(s',a')}.
\end{align*}
$$

On-Policy价值函数的贝尔曼方程与最优价值函数之间的关键区别在于， $\max$ 在动作上的存在与否。它的出现反映了这样一个事实：每当代理选择其动作时，为了采取最优行动，它必须选择能够带来最高价值的动作。

> “贝尔曼备份”这个术语在强化学习文献中经常出现。一个状态（或状态-动作对）的贝尔曼备份是贝尔曼方程的右边：奖励加下一个值。

### 优势函数

在强化学习中，有时我们不需要描述某个动作的绝对优劣，而只需要描述它平均比其他动作好多少。也就是说，我们想知道该动作的相对优势 。我们用优势函数来精确地表述这个概念。

与策略 $\pi$ 对应的优势函数 $A^{\pi}(s,a)$ 描述了在状态 $s$ 下采取特定行动 $a$ 比根据 $\pi(\cdot|s)$ 随机选择行动（假设你此后一直按照 $\pi$ 行动）要好多少。从数学上讲，优势函数定义为

$$
A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s).
$$

优势函数对于策略梯度方法至关重要。

### 形式化

到目前为止，我们已经非正式地讨论了代理的环境，但如果你尝试深入研究相关文献，你很可能会遇到这种设置的标准数学形式： 马尔可夫决策过程 (MDP)。MDP 是一个 5 元组， $\langle S, A, R, P, \rho_0 \rangle$ ，其中

- $S$ 是所有有效状态的集合，
- $A$ 是所有有效动作的集合，
- $R : S \times A \times S \to \mathbb{R}$ 是奖励函数，其中 $r_t = R(s_t, a_t, s_{t+1})$ ，
- $P : S \times A \to \mathcal{P}(S)$ 是转移概率函数，其中 $P(s'|s,a)$ 表示从状态 $s$ 开始并采取行动 $a$ 时转移到状态 $s'$ 的概率，
- 并且 $\rho_0$ 是起始状态分布。

马尔可夫决策过程这个名称指的是系统遵循马尔可夫特性 ：转换仅取决于最近的状态和动作，而不取决于先前的历史。